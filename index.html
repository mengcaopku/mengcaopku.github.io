<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Meng Cao</title>
  
  <meta name="author" content="Meng Cao">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="image/pku_head.png">
</head>

  <body>
  <table width="950" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
    <td>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="67%" valign="middle">
        <p align="center">
          <name>Meng Cao &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
		  </name>
        </p>
		<p>Currently, Meng Cao is a postdoctoral researcher at Mohamed bin Zayed University of Artificial Intelligence (MBZUAI), working with <a href="https://mbzuai.ac.ae/study/faculty/xiaodan-liang/">Prof. Xiaodan Liang</a> and <a href="https://mbzuai.ac.ae/study/faculty/ian-reid/">Prof. Ian D Reid</a>. Prior to that, he worked as a researcher at International Digital Economy Academy (IDEA), supervised by <a href="https://www.leizhang.org/">Lei Zhang</a>. 
      He received the Ph.D. degree from School of Computer Science, Peking University</a>, supervised by <a href="https://scholar.google.com/citations?user=sfyr7zMAAAAJ&hl=zh-CN">Prof. Yuexian Zou</a> (2018 - 2023). 
      He received his B.E. degree from Huazhong University of Science and Technology (2014 - 2018). 
    </p> During his Ph.D. period, he also worked closely with <a href="https://sites.google.com/view/showlab">Prof. Mike Z. Shou</a> from National University of Singapore, <a href="https://zjuchenlong.github.io/">Prof. Long Chen</a> from The Hong Kong University of Science and Technology, and <a href="https://scholar.google.com/citations?user=-ncz2s8AAAAJ&hl=en">Fangyun Wei</a> from Microsoft Research Asia.
		</p> His primary research interests are Computer Vision and Multimedia Analysis. He aims to build an interactive AI assistant that can not only ground and reason over multi-modal signals, but also assist humans in customized content creation.
		</p>


        <p align=center>
          <a href="mailto:mengcaopku[at]gmail.com">Email</a> &nbsp/&nbsp
          <a href="image/cv_mengcao.pdf">CV</a> &nbsp/&nbsp
		  <a href="https://scholar.google.com/citations?user=ZRbRQ0cAAAAJ&hl=en">Google Scholar</a>  &nbsp/&nbsp
          <a href="https://github.com/mengcaopku/">Github</a> 

        </p>
        <!-- </td> -->
        <!-- <td width="33%"> -->
        <!-- <img src="image/self_portrait.jpg" width="210"> -->
        <!-- </td> -->
      </tr>
      </table>


<p></p><p></p><p></p><p></p><p></p>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>News</heading>
          <p>
      <li> <strongsmall>[2025/04]</strongsmall> &nbsp;&nbsp;<smalll>I am invited to serve as Area Chair for NeurIPS 2025.</smalll><br/>
      <li> <strongsmall>[2025/02]</strongsmall> &nbsp;&nbsp;<smalll>I am invited to serve as Area Chair for ACL Rolling Review, February 2025.</smalll><br/>
      <li> <strongsmall>[2024/05]</strongsmall> &nbsp;&nbsp;<smalll>Our paper on parameter-efficient text-video retrieval is accepted by findings of the Association for Computational Linguistics (ACL).</smalll><br/>
      <li> <strongsmall>[2023/08]</strongsmall> &nbsp;&nbsp;<smalll>Our paper on spatio-temporal video grounding is accepted by ACM International Conference on Multimedia Workshop.</smalll><br/>
		  <li> <strongsmall>[2023/08]</strongsmall> &nbsp;&nbsp;<smalll>Our paper on video grounding is accepted by ICCV 2023 as as <font color="red">oral presentation</font>.</smalll><br/>
      <li> <strongsmall>[2023/05]</strongsmall> &nbsp;&nbsp;<smalll>I successfully passed my PhD thesis defense. I'd like to thank everyone who has helped me. Thank you all!</smalll><br/>
      <li> <strongsmall>[2023/03]</strongsmall> &nbsp;&nbsp;<smalll>Our paper on weakly-supervised video grounding is accepted by CVPR 2023.</smalll><br/>
      <li> <strongsmall>[2022/08]</strongsmall> &nbsp;&nbsp;<smalll>Our team win the first place <font color="red">(1/4278)</font> in <a href="https://tianchi.aliyun.com/competition/entrance/532010/introduction">Tianchi Challenge of Financial QA under Market Volatility</a>. </smalll><br/>
		  <li> <strongsmall>[2022/07]</strongsmall> &nbsp;&nbsp;<smalll>Our paper on temporal action localization is accepted by IEEE-TIP.</smalll><br/>
		  <li> <strongsmall>[2022/07]</strongsmall> &nbsp;&nbsp;<smalll>Our paper on video-text pre-training is accepted by ECCV 2022.</smalll><br/>
		  <li> <strongsmall>[2022/06]</strongsmall> &nbsp;&nbsp;<smalll>Our paper on spatio-temporal video grounding is accepted by ACM MM 2022.</smalll><br/>
		  <li> <strongsmall>[2021/08]</strongsmall> &nbsp;&nbsp;<smalll>Our paper on video grounding is accepted by EMNLP 2021 as <font color="red">oral presentation</font>.</smalll><br/>
		  <li> <strongsmall>[2021/06]</strongsmall> &nbsp;&nbsp;<smalll>Our paper on video portrait manipulation is accepted by IEEE-TIP.</smalll><br/>
		  <li> <strongsmall>[2021/03]</strongsmall> &nbsp;&nbsp;<smalll>Our paper on temporal action localization is accepted by CVPR 2021.</smalll><br/>
		  <li> <strongsmall>[2021/03]</strongsmall> &nbsp;&nbsp;<smalll>Our paper on scene text detecton is accepted by IEEE-TCSVT.</smalll><br/>
          
          </p>
        </td>
      </tr>
</table>




      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Education</heading>
        </td>
      </tr>
      </table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td width="10%">
            <img src='image/pku_logo.png' width="100">
          </td>

          <td width="75%" valign="middle">
          <p>
          <stronghuge>Peking University</stronghuge><br />
          Doctoral Student in School of Computer Science &nbsp;&nbsp;&nbsp;&nbsp; &bull; Sep. 2018 - Jun. 2023 <br />
          Supervisors: Prof. <a href="https://scholar.google.com/citations?user=sfyr7zMAAAAJ&hl=en">Yuexian Zou</a>
          </p>
        </td>
      </tr>
	    <tr>
          <td width="10%">
            <img src='image/hust_logo.png' width="100">
          </td>

          <td width="75%" valign="middle">
          <p>
          <stronghuge>Huazhong Univeristy of Science and Technology</stronghuge><br />
          Bachelor Degree &nbsp;&nbsp;&nbsp;&nbsp; &bull; Aug. 2014 - Jun. 2018 <br />
          <!-- GPA: <strong>92.98</strong>/100, &nbsp;&nbsp;Ranking: <strong>2/284</strong> (Overall) or <strong>1/415</strong> (first 2 years)<br /> -->
          <!-- Supervisor: Prof. <a href="https://personal.ntu.edu.sg/hanwangzhang/">Zhang Hanwang</a> -->
          </p>
        </td>
      </tr>
	  
      </table>





<p></p><p></p><p></p><p></p><p></p>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Internship Experience</heading>
        </td>
      </tr>
      </table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td width="10%">
            <a href="https://ai.tencent.com/ailab/en/index/">
            <img src='image/logo_tencentai.png' width="100">
            </a>
          </td>

          <td width="80%" valign="middle">
          <p>
          <stronghuge>Tencent AI Lab</stronghuge><br />
          <huge><em>Engineering and Research Intern</em></huge>&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &bull; Jul. 2019 - May. 2022 <br />
          Advisors: &nbsp; <a href="https://scholar.google.com.hk/citations?user=wTJ83eEAAAAJ&hl=en/">Haozhi Huang</a>, <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=ldPgKEUAAAAJ&view_op=list_works">Hao Wang</a>, <a href="https://zjuchenlong.github.io/">Long Chen</a>, <a href="https://tianyu-yang.com/">Tianyu Yang</a>, and <a href="https://sites.google.com/view/cuweiliu/home">Wei Liu</a><br/>
          </p>
        </td>
      </tr>
	  
	  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td width="10%">
            <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/">
            <img src='image/logo_ms.png' width="100">
          </a>
          </td>

          <td width="80%" valign="middle">
          <p>
          <stronghuge>Micorsoft Research Asia</stronghuge><br />
          <huge><em>Research Intern</em></huge>&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &bull; Jun. 2022 - Mar. 2023 <br />
          Advisors: &nbsp; <a href="https://scholar.google.com/citations?user=-ncz2s8AAAAJ&hl=en">Fangyun Wei</a>, and <a href="https://nlpxucan.github.io/">Can Xu</a>
          </p>
        </td>
      </tr>

<p></p><p></p>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Selected Publication <a href="https://scholar.google.com/citations?user=ZRbRQ0cAAAAJ&hl=en" style="font-size:22px;">[Google Scholar]</a></heading>
        </td>
      </tr>
      </table>


<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
          <td align="center" width="25%">
            <img src='image/acl_rap.jpg'  width="200" height="150">
          </td>
          <td valign="top" width="75%">
       <strong>RAP: Efficient Text-Video Retrieval with Sparse-and-Correlated Adapter
    </strong><br>
       <strong>Meng Cao</strong>, Haoran Tang, Jinfa Huang, Peng Jin, Can Zhang, Ruyang Liu, Long Chen, Xiaodan Liang, Li Yuan, Ge Li<br>  
       <em>Findings of the Association for Computational Linguistics, <strong>ACL 2024 Findings </strong></em><br>
        <!--<em>(Internship Project at Microsoft)</em> <br>-->
        <a href="https://aclanthology.org/2024.findings-acl.427.pdf">[Paperlink]</a>, <a href="https://mengcaopku.github.io/">[Code]</a><br>
            <em>Area: Text-Video Retrieval, Parameter-efficient Fine-tuning</em> <br>
            <p></p>
        <p> We propose RAP to conduct efficient text-video retrieval with a sparse-and-correlated adapter.</p>
          </td>
        </tr>
       </table>
          
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td align="center" width="25%">
        <img src='image/cvpr_iron.jpg'  width="200" height="150">
      </td>
      <td valign="top" width="75%">
	 <strong>Iterative Proposal Refinement for Weakly-Supervised Video Grounding
</strong><br>
	 <strong>Meng Cao</strong>, Fangyun Wei, Can Xu, Xiubo Geng, Long Chen, Can Zhang, Yuexian Zou, Tao Shen, Daxin Jiang<br>  
   <em>IEEE Computer Vision and Pattern Recognition Conference, <strong>CVPR 2023 </strong></em><br>
		<!--<em>(Internship Project at Microsoft)</em> <br>-->
		<a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cao_Iterative_Proposal_Refinement_for_Weakly-Supervised_Video_Grounding_CVPR_2023_paper.pdf">[Paperlink]</a>, <a href="https://mengcaopku.github.io/">[Code]</a><br>
        <em>Area: Video Grounding, Weakly-Supervised Learning</em> <br>
        <p></p>
		<p> We introduce IRON, which includes a novel iterative proposal refinement module to model explicit correspondence for each proposal at both semantic and conceptual levels. </p>
      </td>
    </tr>
   </table>
          
    
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td align="center" width="25%">
        <img src='image/eccv_LocVTP.jpg'  width="200" height="150">
      </td>
      <td valign="top" width="75%">
	 <strong>LocVTP: Video-Text Pre-training for Temporal Localization
</strong><br>
	 <strong>Meng Cao</strong>, Tianyu Yang, Junwu Weng, Can Zhang, Jue Wang, Yuexian Zou<br>
	 <em>European Conference on Computer Vision, <strong>ECCV 2022 </strong></em><br>
		<a href="https://arxiv.org/abs/2207.10362">[Paperlink]</a>, <a href="https://github.com/mengcaopku/LocVTP">[Code]</a><br>
        <em>Area: Video-Langauge Pre-training, Video Retrieval, Temporal Grounding</em> <br>
        <p></p>
		<p> We propose a novel Localization-oriented VideoText Pre-training framework, dubbed as LocVTP, which benefits both retrieval-based and the less-explored localization-based downstream tasks.</p>
      </td>
    </tr>
   </table>
          
          

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td width="15%">
        <img src='image/emnlp_gtr.jpg'  width="200" height="173">
      </td>
      <td valign="top" width="75%">
	 <strong>On Pursuit of Designing Multi-modal Transformer for Video Grounding</strong><br>
	 <strong>Meng Cao</strong>,  Long Chen, Mike Zheng Shou, Can Zhang, Yuexian Zou<br>  
        <em>Empirical Methods in Natural Language Processing, <strong>EMNLP 2021 </strong></em><br>
        <em><strong><font color="#a82e2e">(Oral Presentation)</font></strong></em> <br>
		
		<a href="https://aclanthology.org/2021.emnlp-main.773.pdf">[Paperlink]</a>, <a href="https://github.com/mengcaopku">[Code]</a></em><br>
        <em>Area: Video Grounding, Transformer Architecture Design</em> <br>
        <p></p>
		<p>We propose the first end-to-end model GTR for video grounding, which is inherently efficient with extremely fast inference speed. Our comprehensive explorations and empirical results can help to guide the design of more multi-modal Transformerfamily models in other multi-modal tasks..</p>
      </td>
    </tr>
   </table>
   
   
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td width="15%">
        <img src='image/acm_DCNet.jpg'  width="200" height="165">
      </td>
      <td valign="top" width="75%">
	 <strong>Correspondence Matters for Video Referring Expression Comprehension</strong><br>
	 <strong>Meng Cao</strong>, Ji Jiang, Long Chen, Yuexian Zou<br>
        <em>ACM Multimedia, <strong>ACM MM 2022</strong></em><br>
		
		<a href="https://arxiv.org/abs/2207.10400">[Paperlink]</a>, <a href="https://github.com/mengcaopku/DCNet">[Code]</a></em><br>
        <em>Area: Referring Expression Comprehension, Correspondence Modeling</em> <br>
        <p></p>
		<p>We propose a novel Dual Correspondence Network (dubbed as DCNet) which explicitly enhances the dense associations in both the inter-frame and cross-modal manners.</p>
      </td>
    </tr>
   </table>
   
   

 <!-- , bgcolor="#ffffeb"> -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()">
      <td width="15%">
        <img src='image/tip_dmp.jpg'  width="200" height="160">
      </td>
      <td valign="top" width="75%">
	 <strong>Deep Motion Prior for Weakly-Supervised Temporal Action Localization</strong><br>
	 <strong>Meng Cao</strong>, Can Zhang, Long Chen, Mike Zheng Shou, Yuexian Zou<br>
        <em>IEEE Transactions on Image Processing, <strong>TIP 2022</strong></em><br>
		
		<a href="https://arxiv.org/pdf/2108.05607.pdf">[Paperlink]</a>, 
		<a href="https://github.com/">[Code]</a>, <a href="https://www.youtube.com/watch?v=WVJZjiUku1E">[Video]</a></em><br>
		
        <em>Area: Temporal Action Localization, Weakly-Supervised Learning, Graph Network.</em> <br>
        <p></p>
		<p>We establish a context-dependent deep motion prior with a novel motion graph and propose an efficient motion-guided loss to inform the whole pipeline of more motion cues.</p>
      </td>
    </tr>
   </table>
   
   
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td width="15%">
        <img src='image/tip_unifacegan.jpg'  width="200" height="132">
      </td>
      <td valign="top" width="75%">
	 <strong>UniFaceGAN: A UniFied Framework for Temporally Consistent Facial Video Editing</strong><br>
     <strong>Meng Cao</strong>, Haozhi Huang, Hao Wang, Xuan Wang, Li Shen, Sheng Wang, Linchao Bao, Zhifeng Li, Jiebo Luo<br>
        <em>IEEE Transactions on Image Processing, <strong>TIP 2021</strong></em><br>
		<a href="https://arxiv.org/abs/2108.05650">[Paperlink]</a>, 
		<a href="https://github.com/">[Code]</a>, <a href="https://www.youtube.com/watch?v=ohnmCZwF1Vs">[Video]</a></em><br>
        <em>Area: Facial Video Manipulation, Generative Adversarial Network</em> <br>
        <p></p>
		<p>We present a unified framework that offers solutions for multiple tasks, including face swapping, face reenactment, and ``fully disentangled manipulation".</p>
      </td>
    </tr>
   </table>
   
   
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td width="15%">
        <img src='image/tcsvt_nask_img.jpg'  width="200" height="170">
      </td>
      <td valign="top" width="75%">
	 <strong>All You Need is a Second Look: Towards Arbitrary-Shaped Text Detection</strong><br>
    <strong>Meng Cao</strong>, Can Zhang, Dongming Yang, Yuexian Zou<br>
        <em>IEEE Transactions on Circuits and Systems for Video Technology, <strong>TCSVT 2021</strong></em><br>
		<a href="https://arxiv.org/pdf/2106.12720.pdf">[Paperlink]</a><br>
        <em>Area: Scene Text Detection</em> <br>
        <p></p>
		<p> We propose a two-stage segmentation-based scene text detector, which conducts the detection in a coarse-to-fine manner. Besides, we establish a much tighter representation for arbitrary-shapedtexts.</p>
      </td>
    </tr>
   </table>
   
   

    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()">
      <td width="15%">
        <img src='image/WAIC_Graph.jpg'  width="200" height="121">
      </td>
      <td valign="top" width="75%">
	 <strong>Technical Report for WAIC Challenge of Financial QA under Market Volatility</strong><br>
       <strong>Meng Cao</strong>, Ji Jiang, Qichen Ye, Yuexian Zou<br>
        <em>Technical Report for TianChi Challenge</em><br>
		<a href="https://www.researchgate.net/profile/Meng_Cao31/publication/364028242_Technical_Report_for_WAIC_Challenge_of_Financial_QA_under_Market_Volatility/links/63369162ff870c55ceea2089/Technical-Report-for-WAIC-Challenge-of-Financial-QA-under-Market-Volatility.pdf">[Paperlink]</a></em><br>
        <em><strong><font color="#a82e2e">Winner of Financial QA Challenge</font></strong></em> <br>
        <p></p>
		<p>We address the problem of financial QA by proposing a graph transformer model for the efficient multi-source information fusion. As a result, we won the first place out of 4278 participating teams and outperformed the second place by 5.07 times on BLUE. </p>
      </td>
    </tr>
   </table>




<p></p><p></p><p></p><p></p><p></p>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Academic Service</heading>
          <div style="line-height:25px">
          <p>
		  <!-- <li> <stronghuge>Co-organizer:</stronghuge> &nbsp; <a href="https://nicochallenge.com/">NICO Challenge 2022</a> (ECCV'22 Workshop)<br/> -->
		  <li> <stronghuge>Conference Reviewer:</stronghuge> &nbsp; ICCV'21, CVPR'22, ECCV'22, EMNLP'22, AAAI'23, CVPR'23, ICCV'23, NeurIPS'23<br/>
		  <li> <stronghuge>Journal Reviewer:</stronghuge> &nbsp; IEEE TCSVT, ACM ToMM, IEEE TIP<br/>
          </p>
          </div>
        </td>
      </tr>
</table>



<!-- 
<p></p><p></p><p></p><p></p><p></p>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Talk</heading>
          <div style="line-height:25px">
          <p>
          <li> "Equivariant Similarity for Vision-Language Foundation Models (VLM)", Microsoft, Reading Group, 2023.06<br/>
          <li> "Equivariant Similarity for Vision-Language Foundation Models (VLM)", KAUST, Rising Stars in AI Symposium, 2023.03<br/>
          <li> "Equivariance and Invariance Inductive Bias for Learning from Insufficient Data", JiangMen Talk, 2022.10<br/>
		  <li> "Self-Supervised Learning Disentangled Group Representation as Feature", PREMIA AGM 2022, 2022.08<br/>
		  <li> "Towards Out-of-Distribution Generalization in Computer Vision", National University of Singapore (NUS), 2022.04<br/>
		  <li> "Disentangled Group Representation Learning and its Potential in Causality", ZhiYuan Community, 2022.01<br/>
		  <li> "Generalization Powered by Invariant Learning", Singapore Management University (SMU), 2021.11<br/>
		  <li> "因果推理的应用与发展 (中文)", AI Time, 2021.10 &nbsp<a href="https://www.bilibili.com/video/BV1qP4y1b7us?spm_id_from=333.999.0.0">[Video]</a><br/>
          </p>
          </div>
        </td>
      </tr>
</table>
-->


<p></p><p></p><p></p><p></p><p></p>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Honors & Scholarships</heading>
          <div style="line-height:25px">
          <p>
		  <li> <stronghuge>Outstanding graduate of Peking University</stronghuge>,&nbsp; 2023<br/>
		  <li> <stronghuge>Outstanding graduates of Beijing City</stronghuge>,&nbsp; 2023<br/>
      <li> <stronghuge>May Fourth Scholarship (Annual Highest Honor Scholarship in PKU)</stronghuge>,&nbsp; 2022<br/>
          <li> <stronghuge>The first place (1/4278) in Challenge of Financial QA under Market Volatility</stronghuge>,&nbsp; 2022<br/>
          <li> <stronghuge>Excellent Student Scholarship at PKU</stronghuge>,&nbsp; 2018~2023<br/>
      		  <li> <stronghuge>Academic Innovation Award of Peking University</stronghuge>,&nbsp; 2020<br/>
          <li>  <stronghuge>Excellent Graduate at HUST (Bachelor degree)</stronghuge>,&nbsp; 2018 <br/>
          <li> <stronghuge>National Endeavor Scholarship</stronghuge> (Top 5% students per year),&nbsp; 2017<br/>
          <li> <stronghuge>Honorable Mention Price, Mathematical Contest in Modeling</stronghuge>,&nbsp; 2017<br/>
		  <li> <stronghuge>Third prize in National Mathematical Modeling Contest</stronghuge>,&nbsp; 2016<br/>
          </p>
          </div>
        </td>
      </tr>
</table>


<!--
<p></p><p></p><p></p><p></p><p></p>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Project</heading>
		  <p>
		  <div style="text-align: center;">
		  <img src='project/projects_icon.png'  width="600">
		  </p>
          </div>
        </td>
      </tr>
</table>
-->

<!-- 
<p></p><p></p><p></p><p></p><p></p>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Leadership Experience</heading>
      </td>
      </tr>
      </table>


    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td width="15%">
        <img src='image/lecture.jpg'  width="195" height="130">
      </td>
      <td valign="top" width="75%">
        <stronghuge>Lecture Group of EE Department</stronghuge> </br>
        <huge><em>Founder & President</em></huge>&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &bull; Oct. 2017 - Sep. 2018 </br>
        <p></p>
        <p>
         <li> Organized academic forum, sharing sessions, Q&A meetings more than 30 times, serving over 1000 students on studying and future planing.<br/>
          <li> The team grows to 30 people and won the Outstanding Student Organisation prize in 2018.<br/>
          </p> 
      </td>
    </tr>
   </table>


       <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="dachuang_stop()" onmouseover="dachuang_start()" >
      <td width="26%">
        <div class="one">
                <div class="two" id='dachuang_image'><img src='image/dachuang2.png'  width="195" height="130"></div>
                <img src='image/dachuang1.png'  width="195" height="130">
              </div>
      <script type="text/javascript">
                function dachuang_start() {
                  document.getElementById('dachuang_image').style.opacity = "1";
                }
                function dachuang_stop() {
                  document.getElementById('dachuang_image').style.opacity = "0";
                }
                dachuang_stop()
              </script>
      </td>

      <td valign="top" width="75%">
        <stronghuge>Innovative Entrepreneurship Project of UESTC</stronghuge> </br>
        <huge><em>Team Leader</em></huge>&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &bull; Sep. 2017 - Mar. 2018 </br>
        <p></p>
        <p>
         <li> This project focus on the pedestrian detection in low-light condition with excellent conclusion. We combine the recent pedestrian detection models with the low-light image enhancement algorithm based on Laplace operator.<br/>
         <li> Responsible for the code implementation and project promotion.<br/>
          </p> 
      </td>
    </tr>
   </table>
-->
       



<!-- 
<p></p><p></p><p></p><p></p><p></p>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Personal Interests</heading>
          <p>
          <stronghuge>DOTA1</stronghuge>: My first and most playing PC game which accompanied me in my whole middle and high school. And I got about 1350 score  on the '11' Battle Platform Ladder Tournament. :)
          </p>
          <p>
          <stronghuge>Running</stronghuge>: During my college, I offen run a long distance for the pleasure releasing. And I have participated in the Chengdu Shuangyi Marathon in 2018.
          </p>
      </td>
      </tr>
-->



<!-- 
   <p></p><p></p><p></p><p></p><p></p>
   <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=250&t=tt&d=85Rlf3OqLYVhTE6hGEcHnAsDJl6O0EsUp326ZMpLzCI"></script>
-->

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
			<tbody><tr>
				<td>
				<br>
				<p align="left"><font size="2">
				Last updated on Oct, 2023
				<p align="middle"><font size="2">
				This awesome template borrowed from <a href="https://people.eecs.berkeley.edu/~barron/">this guy</a>~
				</tbody></table>
   

</body>
</html>
